{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python370jvsc74a57bd0c751e3dba1c0ae6d681c8fd9e4eb9597bc490d2dc541e0c49930039039a54e24",
   "display_name": "Python 3.7.0 64-bit ('U4-S1-NLP': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Objective 01 - describe the latent dirichlet allocation process\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Objective 02 - implement a topic model using the gensim library\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Topic Modeling Steps\n",
    "\n",
    "Prepare text: Load text file, split into documents, tokenize/lemmatize, remove stop words\n",
    "\n",
    "Create the term dictionary for the corpus\n",
    "\n",
    "Create a document term matrix (DTM)\n",
    "\n",
    "Set-up the LDA model, decide on number of topics\n",
    "\n",
    "Run and train the model\n",
    "\n",
    "Topics!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# add additional stop words\n",
    "# STOPWORDS = set(STOPWORDS).union(set(['said', 'mr', 'mrs'])))\n",
    "\n",
    "# Function for tokenizing \n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['alice',\n",
       " 'beginning',\n",
       " 'tired',\n",
       " 'sitting',\n",
       " 'sister',\n",
       " 'bank',\n",
       " 'having',\n",
       " 'twice',\n",
       " 'peeped',\n",
       " 'book']"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# read in the text \n",
    "with open('wonderland.txt', 'r',) as file:\n",
    "    text_str = file.read()\n",
    "\n",
    "# Split the string on the newline character\n",
    "text = text_str.split('\\n')\n",
    "\n",
    "# Tokenize each chunk of text\n",
    "text_tokens = [tokenize(chunk) for chunk in text]\n",
    "\n",
    "# look at the first 10 tokens\n",
    "text_tokens[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 1), (1, 4), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from gensim import corpora\n",
    "\n",
    "# create the term dictionary of our corpus\n",
    "# every unique term is assigned an index\n",
    "dictionary = corpora.Dictionary(text_tokens)\n",
    "\n",
    "# Convert the list of documents (corpus) into Document Term Matrix\n",
    "# using the dictionary we just created\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_tokens]\n",
    "\n",
    "# what does this matrix looks like\n",
    "print(doc_term_matrix[0][0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(1, '0.022*\"little\" + 0.018*\"alice\" + 0.014*\"thing\" + 0.014*\"cakes\" + 0.009*\"thought\"'), (3, '0.017*\"time\" + 0.017*\"executioner\" + 0.013*\"duchess\" + 0.013*\"went\" + 0.013*\"alice\"'), (4, '0.021*\"rabbit\" + 0.021*\"alice\" + 0.014*\"little\" + 0.014*\"sister\" + 0.011*\"eyes\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Create the object for LDA model\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Train LDA model on the document term matrix\n",
    "# topics = 5\n",
    "ldamodel = lda(doc_term_matrix, num_topics=5, id2word=dictionary, passes=50)\n",
    "\n",
    "# Print out the topics\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------Topic 0-------\nlittle rabbit gloves fan alice\n \n-------Topic 1-------\nalice little ran said went\n \n-------Topic 2-------\nsaid turtle mock executioner alice\n \n-------Topic 3-------\nalice came rabbit hearts thought\n \n-------Topic 4-------\nalice little look thought king\n \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "words = [re.findall(r'\"([^\"]*)\"', t[1]) for t in ldamodel.print_topics()]\n",
    "topics = [' '.join(t[0:5]) for t in words]\n",
    "for id, t in enumerate(topics):\n",
    "    print(f\"-------Topic {id}-------\")\n",
    "    print(t, end=\"\\n \\n\")"
   ]
  },
  {
   "source": [
    "## Objective 03 - interpret document topic distributions and summarize findings\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\u001b[0m\n\u001b[31mERROR: No matching distribution found for warnings\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:169: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:286: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:858: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True):\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1094: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1120: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, positive=False):\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1349: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1590: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/linear_model/_least_angle.py:1723: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/sklearn/decomposition/_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "# suppress annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# import the library\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# Use the visualization in a notebook\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# repeat the topic model from the previous objective \n",
    "\n",
    "#imports\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "# additional stop words\n",
    "# STOPWORDS = set(STOPWORDS).union(set(['said', 'mr', 'mrs']))\n",
    "\n",
    "# Function for tokenizing the text\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "# Read the file text\n",
    "with open('wonderland.txt', 'r') as file:\n",
    "    text_str = file.read()\n",
    "\n",
    "# split the string on the newline character\n",
    "text = text_str.split('\\n')\n",
    "\n",
    "# Tokenize each chunk of text\n",
    "text_tokens = [tokenize(chunk) for chunk in text]\n",
    "\n",
    "# imports\n",
    "from gensim import corpora\n",
    "\n",
    "# Create the term dictionary of our corpus\n",
    "# every unique term is assigned as index\n",
    "dictionary = corpora.Dictionary(text_tokens)\n",
    "\n",
    "# Convert eht list of documents(corpus) into Document Term Matrix\n",
    "# using the dictionary we just created\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_tokens]\n",
    "\n",
    "# import \n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Create the object for LDA model\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Train LDA model on the document term matrix\n",
    "# topic = 5\n",
    "ldamodel = lda(doc_term_matrix, num_topics=5, id2word=dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rob/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.112162  0.023463       1        1  43.948916\n",
       "1      0.104811  0.078985       2        1  23.600776\n",
       "3     -0.032671  0.051671       3        1  11.596795\n",
       "2      0.054883 -0.089599       4        1  10.884468\n",
       "4     -0.014860 -0.064520       5        1   9.969046, topic_info=        Term       Freq      Total Category  logprob  loglift\n",
       "420   turtle   4.000000   4.000000  Default  30.0000  30.0000\n",
       "75      came   7.000000   7.000000  Default  29.0000  29.0000\n",
       "392     mock   3.000000   3.000000  Default  28.0000  28.0000\n",
       "183     said  10.000000  10.000000  Default  27.0000  27.0000\n",
       "91    gloves   5.000000   5.000000  Default  26.0000  26.0000\n",
       "..       ...        ...        ...      ...      ...      ...\n",
       "123  talking   0.575324   2.668059   Topic5  -5.1377   0.7715\n",
       "30    looked   0.575318   3.965961   Topic5  -5.1378   0.3751\n",
       "290    hands   0.575314   1.758552   Topic5  -5.1378   1.1884\n",
       "289   guests   0.575313   1.758551   Topic5  -5.1378   1.1884\n",
       "57   thought   0.575302   6.129627   Topic5  -5.1378  -0.0603\n",
       "\n",
       "[294 rows x 6 columns], token_table=      Topic      Freq   Term\n",
       "term                        \n",
       "1         1  0.562776  alice\n",
       "1         2  0.204646  alice\n",
       "1         3  0.102323  alice\n",
       "1         4  0.102323  alice\n",
       "1         5  0.102323  alice\n",
       "...     ...       ...    ...\n",
       "67        2  0.132062  white\n",
       "67        3  0.132062  white\n",
       "67        5  0.132062  white\n",
       "269       2  1.018990   wood\n",
       "500       4  0.625643  words\n",
       "\n",
       "[362 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 4, 3, 5])"
      ],
      "text/html": "\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el12081404839855222645483598900\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el12081404839855222645483598900_data = {\"mdsDat\": {\"x\": [-0.11216171742562353, 0.10481056660713117, -0.032670967672035776, 0.05488260724240972, -0.01486048875188147], \"y\": [0.02346288224063516, 0.07898462134362964, 0.0516712826315779, -0.08959905412727134, -0.06451973208857133], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [43.94891645615612, 23.600775738893574, 11.596794548848807, 10.884467735846599, 9.969045520254898]}, \"tinfo\": {\"Term\": [\"turtle\", \"came\", \"mock\", \"said\", \"gloves\", \"fan\", \"hearts\", \"gryphon\", \"king\", \"procession\", \"went\", \"rabbit\", \"little\", \"oh\", \"knave\", \"like\", \"ll\", \"queen\", \"know\", \"nearly\", \"come\", \"hand\", \"sat\", \"ornamented\", \"lie\", \"soldiers\", \"carrying\", \"gardeners\", \"hunting\", \"pair\", \"executioner\", \"away\", \"day\", \"stupid\", \"argument\", \"read\", \"rest\", \"ask\", \"court\", \"time\", \"king\", \"pictures\", \"conversations\", \"book\", \"waistcoat\", \"cut\", \"knew\", \"near\", \"mind\", \"cat\", \"going\", \"started\", \"wasn\", \"things\", \"party\", \"kept\", \"trial\", \"question\", \"watch\", \"scroll\", \"dear\", \"rabbit\", \"think\", \"round\", \"look\", \"white\", \"alice\", \"oh\", \"large\", \"went\", \"queen\", \"duchess\", \"great\", \"began\", \"looked\", \"thought\", \"got\", \"hand\", \"little\", \"said\", \"cakes\", \"coaxing\", \"eat\", \"idea\", \"guinea\", \"size\", \"wood\", \"plan\", \"pigs\", \"dream\", \"knee\", \"trees\", \"possibly\", \"doubt\", \"touch\", \"set\", \"case\", \"spite\", \"turning\", \"feebly\", \"lovely\", \"second\", \"smaller\", \"small\", \"whistle\", \"delighted\", \"puppy\", \"lay\", \"pebbles\", \"enormous\", \"thing\", \"frightened\", \"bright\", \"little\", \"head\", \"poor\", \"lizard\", \"voice\", \"eyes\", \"hear\", \"soon\", \"alice\", \"way\", \"sister\", \"hard\", \"looking\", \"ran\", \"began\", \"thought\", \"mary\", \"ann\", \"ferrets\", \"mistake\", \"called\", \"guessed\", \"angry\", \"vanished\", \"brass\", \"upstairs\", \"explain\", \"housemaid\", \"meet\", \"swim\", \"whiskers\", \"knocking\", \"engraved\", \"plate\", \"run\", \"home\", \"paws\", \"glass\", \"fear\", \"lest\", \"slowly\", \"turned\", \"direction\", \"lost\", \"fur\", \"quick\", \"fan\", \"gloves\", \"house\", \"door\", \"hunting\", \"pair\", \"ll\", \"went\", \"moment\", \"rabbit\", \"oh\", \"looking\", \"ran\", \"great\", \"duchess\", \"alice\", \"white\", \"said\", \"little\", \"hurried\", \"soon\", \"good\", \"table\", \"seen\", \"executed\", \"better\", \"fetch\", \"turtle\", \"words\", \"deep\", \"sorrow\", \"mock\", \"gryphon\", \"sit\", \"saying\", \"lady\", \"history\", \"hasn\", \"hollow\", \"deeply\", \"finish\", \"occasional\", \"broken\", \"constant\", \"wants\", \"speak\", \"sighing\", \"lonely\", \"sad\", \"exclamation\", \"patiently\", \"saw\", \"sigh\", \"pitied\", \"interesting\", \"heavy\", \"fancy\", \"heart\", \"sat\", \"know\", \"nearly\", \"come\", \"said\", \"alice\", \"tell\", \"thinking\", \"getting\", \"hear\", \"sitting\", \"spoke\", \"begin\", \"gone\", \"ll\", \"distance\", \"help\", \"sir\", \"ve\", \"got\", \"looked\", \"large\", \"till\", \"long\", \"procession\", \"ornamented\", \"lie\", \"carrying\", \"soldiers\", \"gardeners\", \"hearts\", \"people\", \"reply\", \"grand\", \"rule\", \"royal\", \"clubs\", \"couples\", \"faces\", \"diamonds\", \"doubtful\", \"manner\", \"bowed\", \"stopped\", \"stood\", \"velvet\", \"face\", \"noticing\", \"recognised\", \"shaped\", \"courtiers\", \"cushion\", \"severely\", \"crimson\", \"came\", \"knave\", \"like\", \"said\", \"hand\", \"queen\", \"alice\", \"king\", \"hurried\", \"crown\", \"having\", \"ought\", \"feet\", \"couldn\", \"children\", \"remember\", \"heard\", \"use\", \"talking\", \"looked\", \"hands\", \"guests\", \"thought\"], \"Freq\": [4.0, 7.0, 3.0, 10.0, 5.0, 5.0, 3.0, 3.0, 7.0, 2.0, 7.0, 11.0, 12.0, 4.0, 2.0, 2.0, 2.0, 5.0, 2.0, 2.0, 2.0, 4.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 4.142027949906506, 3.345472708368452, 2.5489162597864756, 2.5489162597864756, 2.5489158574385016, 2.5489158574385016, 2.548916058612489, 2.5489158574385016, 2.5489158574385016, 6.531854613710287, 5.735021349722166, 1.7523584029865904, 1.7523584029865904, 1.7523584029865904, 1.7523584029865904, 1.7523582018126034, 1.7523584029865904, 1.7523584029865904, 1.7523582018126034, 1.7523582018126034, 1.7523582018126034, 1.7523582018126034, 1.7523582018126034, 1.7523582018126034, 1.7523582018126034, 1.7523582018126034, 1.7523580006386164, 1.7523582018126034, 1.7523582018126034, 1.7523580006386164, 3.3454608391032177, 8.125035105333879, 3.345523203039195, 3.3455109314259865, 3.3455103279040252, 4.938632680245368, 10.513902169219556, 3.345338927667081, 3.345501073900622, 4.141790564601817, 3.3453851976840965, 3.3454197996098647, 3.3453602521097054, 3.3454282489173197, 2.548826335014276, 3.3453445605387175, 2.5488826637306428, 2.548735404372141, 4.141191066120485, 3.344318372030907, 2.1873820301013374, 1.5038137888957082, 1.5038137888957082, 1.5038137888957082, 1.5038138969270896, 1.5038137888957082, 1.5038137888957082, 1.5038137888957082, 1.5038137888957082, 1.5038137888957082, 1.5038137888957082, 0.8202423067486395, 0.8202423067486395, 0.8202423067486395, 0.8202423067486395, 0.8202421987172581, 0.8202423067486395, 0.8202421987172581, 0.8202423067486395, 0.8202421987172581, 0.8202421987172581, 0.8202423067486395, 0.8202423067486395, 0.8202423067486395, 0.8202421987172581, 0.8202421987172581, 0.8202421987172581, 0.8202421987172581, 0.8202421987172581, 0.8202421447015675, 2.1875805917802027, 1.5038198386530621, 1.5038196225902996, 6.289913275198735, 2.8711336005610657, 1.5040038160954494, 1.504002303656111, 1.5039199837435453, 2.187564171010242, 1.503942562302241, 1.5040138630139122, 4.238229247110185, 1.5039150143000046, 1.5038981614045188, 1.5038921116471649, 1.5038393923330808, 1.5038305337598128, 1.5038480348435865, 1.5038961088082736, 1.1378503372452118, 1.1378503372452118, 1.1378502310777072, 0.6206358035258213, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.620635750442069, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 0.6206356973583166, 2.1725202987520422, 2.1725202987520422, 1.137912339067927, 1.137912339067927, 1.1380782788776937, 1.137995574391572, 1.1381136326567418, 1.655367342185348, 1.1380297603280691, 1.6553738184031317, 1.1379381377715567, 1.1380260444654062, 1.137991327691386, 1.138014366039895, 1.138011924187288, 1.6551521406534242, 1.1380409079160572, 1.1378751804412999, 1.1379328293963242, 0.6207245064759557, 0.6207217992045871, 0.6207144736467664, 0.6207144736467664, 0.6206997163636201, 0.6206943549046353, 0.6206943549046353, 0.6206944079883876, 3.1087895790842492, 1.1030016828703135, 1.1030015832240903, 1.1030013839316437, 2.607363007594982, 2.1059368346906076, 0.6016281231718313, 0.6016281231718313, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280733487197, 0.6016280235256081, 0.6016279737024964, 0.6016279737024964, 0.6016279737024964, 0.6016279737024964, 0.6016279737024964, 0.6016279737024964, 0.6016279238793848, 0.6016279238793848, 0.6016279238793848, 1.1030829941885103, 1.1032282783820528, 1.10319599300571, 1.1031502553892243, 2.6077875005061557, 1.6044626323066125, 0.6017202959283656, 0.6017065945726645, 0.6016916974622841, 0.6016918967547307, 0.6016915479929492, 0.6016867649742318, 0.6016867649742318, 0.6016865158586736, 0.6016866653280085, 0.601680786200835, 0.601680786200835, 0.6016808360239466, 0.6016770494674619, 0.6016741099038752, 0.6016565223454662, 0.6016559244681265, 0.6016537322512144, 0.6016537322512144, 1.5341335161104055, 1.0547120008806632, 1.0547119096150395, 1.0547118183494157, 1.0547118183494157, 1.0547117270837922, 2.014001959099068, 0.5752890254009427, 0.5752889797681309, 0.5752889797681309, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888885025073, 0.5752888428696954, 0.5752888428696954, 0.5752888428696954, 0.5752888428696954, 2.9733365462074284, 1.0549151581588654, 1.0548599424565688, 1.5343795682317143, 1.0549168922057144, 1.054885131768691, 1.5342220437653273, 1.0547838269264607, 0.5753774161574292, 0.5753709819309633, 0.5753526831734253, 0.5753528200718607, 0.5753510860250117, 0.5753472072360074, 0.5753436935094975, 0.5753434197126267, 0.5753432371813795, 0.5753414118689069, 0.5753237063379224, 0.5753180935020691, 0.5753135302208876, 0.5753133020568285, 0.5753015744241919], \"Total\": [4.0, 7.0, 3.0, 10.0, 5.0, 5.0, 3.0, 3.0, 7.0, 2.0, 7.0, 11.0, 12.0, 4.0, 2.0, 2.0, 2.0, 5.0, 2.0, 2.0, 2.0, 4.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 4.57834699283206, 3.781791226232745, 2.9852340109605895, 2.985234252339122, 2.985233870122455, 2.985233908204963, 2.9852341886521767, 2.9852341066239605, 2.9852341258385158, 7.651568385709067, 7.130241630537327, 2.1886756871697473, 2.1886757517796203, 2.188675773606012, 2.18867578146505, 2.1886755622457885, 2.188675841769918, 2.1886758616763253, 2.1886756327302086, 2.1886756429215843, 2.188675655142927, 2.188675690530791, 2.1886756966447094, 2.188675704618706, 2.1886757145713465, 2.188675748854903, 2.188675502051583, 2.1886757539790107, 2.188675756015824, 2.188675527778299, 4.29898065081142, 11.275665235480659, 4.465328457458794, 4.4653267585582626, 4.46532711684423, 7.572204795075608, 19.54596823305511, 4.816152492332645, 4.966670615646558, 7.110654151409817, 5.424064748970379, 5.499670580957348, 5.499661289045363, 5.666079475646485, 3.9659605461083247, 6.129627240874948, 4.170098487110656, 4.6274815396351725, 12.745594220359038, 10.127882293250696, 2.6462971711145786, 1.9627279746339363, 1.9627279927293104, 1.962727996802014, 1.9627281570115218, 1.9627281029824573, 1.962728164098642, 1.962728169571398, 1.9627282479416923, 1.9627283105299935, 1.9627283520953296, 1.279155945047954, 1.2791559471427925, 1.2791559480752297, 1.2791559637913368, 1.2791558019764924, 1.2791559709492044, 1.2791558066316298, 1.2791559856232841, 1.2791558213657368, 1.2791558217649917, 1.2791559914435928, 1.2791559978438696, 1.2791560017438743, 1.279155851572425, 1.2791558527351383, 1.27915587496911, 1.2791558841097754, 1.2791558877745879, 1.2791558182151854, 3.4427496400812463, 2.4799237027936702, 2.479923550680454, 12.745594220359038, 5.719443681020681, 2.7591819264287403, 2.7591823199244545, 2.759196392438982, 4.740666319428977, 3.2605103280910965, 3.2763284812755575, 19.54596823305511, 3.555761802090092, 3.5557642729546104, 3.555765346510847, 5.3866466873692, 5.386667111489257, 5.666079475646485, 6.129627240874948, 1.630034453420174, 1.6300344706542331, 1.6300344729864116, 1.1128198506647242, 1.1128198089891748, 1.1128198227277204, 1.1128198227277204, 1.1128198227277204, 1.1128198227277204, 1.1128198353010945, 1.1128198353010945, 1.1128198353010945, 1.1128198353010945, 1.1128198353010945, 1.1128198478744686, 1.1128198478744686, 1.1128198478744686, 1.1128198581175004, 1.1128198592826715, 1.1128198038685766, 1.1128198050337479, 1.1128198061989192, 1.1128198061989192, 1.1128198073640905, 1.1128198073640905, 1.1128198073640905, 1.1128198176071222, 1.1128198199374646, 1.1128198199374646, 1.1128198301804963, 5.0539350551887345, 5.053935611764124, 2.3135443054875218, 2.3135443459400196, 2.4263965347246956, 2.426441649671592, 2.9276989883529074, 7.110654151409817, 3.9065299657895878, 11.275665235480659, 4.816152492332645, 5.3866466873692, 5.386667111489257, 5.499661289045363, 5.499670580957348, 19.54596823305511, 7.572204795075608, 10.127882293250696, 12.745594220359038, 3.0721400000942864, 3.2763284812755575, 1.909262785185581, 1.9092628026533185, 1.9092708868327943, 1.9092737861297, 1.909273783926036, 1.909273726061597, 4.287265902587184, 1.5983553265609471, 1.5983555288883875, 1.5983555358830293, 3.785913089249729, 3.284561113771234, 1.0969808943396946, 1.0969809246084308, 1.0969808445165832, 1.0969808759495463, 1.0969808952713826, 1.0969809036542615, 1.0969809204181311, 1.0969809234443992, 1.0969809360177734, 1.0969809478952217, 1.096980948356525, 1.0969809506868673, 1.0969809632602416, 1.0969809683822294, 1.096980972339046, 1.0969809423060721, 1.0969808987691756, 1.0969809069174317, 1.096980909247774, 1.0969809171604634, 1.0969809360209968, 1.09698095418466, 1.0969809038923028, 1.0969809101794623, 1.0969809101794623, 2.2818559928930497, 2.3947075282451724, 2.3947260330787845, 2.3947534274933355, 10.127882293250696, 19.54596823305511, 1.8934117441878642, 2.576950936907035, 1.8934291076133554, 3.2605103280910965, 1.893429175369372, 1.8934322591129527, 1.8934322736814126, 1.8934322545824278, 2.9276989883529074, 1.8934352655038473, 1.8934353695767734, 1.8934352577046496, 3.3735356815078807, 4.170098487110656, 3.9659605461083247, 4.966670615646558, 1.780501639653696, 1.7805016641028804, 2.033877347136393, 1.554455258776195, 1.5544551776246456, 1.554455226772649, 1.5544552374747047, 1.554455259369449, 3.3094881186799627, 1.075031664541728, 1.0750316671061233, 1.075031674264561, 1.075031594641763, 1.0750316026131526, 1.0750316091938303, 1.0750316123948813, 1.0750316203662709, 1.0750316329396452, 1.07503163398779, 1.0750316343405788, 1.0750316345757713, 1.0750316346933677, 1.0750316402156788, 1.0750316403332751, 1.0750316406860638, 1.0750316471389239, 1.0750316710125565, 1.0750316912147533, 1.0750316193768266, 1.0750316250269558, 1.075031639814216, 1.0750316444059786, 7.5634353859649, 2.350806236328018, 2.3508432758625935, 10.127882293250696, 4.6274815396351725, 5.424064748970379, 19.54596823305511, 7.130241630537327, 3.0721400000942864, 1.8714628308322765, 1.871475113458897, 1.8714751314942522, 3.351577528256372, 1.8714788944300902, 1.8714814912775657, 1.8714816457883696, 2.3886454037986886, 2.6680470294949488, 2.6680593596742046, 3.9659605461083247, 1.7585515157784342, 1.758551362760274, 6.129627240874948], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.647299766540527, -4.860899925231934, -5.132800102233887, -5.132800102233887, -5.132800102233887, -5.132800102233887, -5.132800102233887, -5.132800102233887, -5.132800102233887, -4.191800117492676, -4.321899890899658, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -5.507500171661377, -4.860899925231934, -3.9735000133514404, -4.860799789428711, -4.860799789428711, -4.860799789428711, -4.471399784088135, -3.7158000469207764, -4.860899925231934, -4.860899925231934, -4.647299766540527, -4.860899925231934, -4.860899925231934, -4.860899925231934, -4.860899925231934, -5.132800102233887, -4.860899925231934, -5.132800102233887, -5.132900238037109, -4.647500038146973, -4.861199855804443, -4.664000034332275, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -5.644899845123291, -4.663899898529053, -5.038700103759766, -5.038700103759766, -3.607800006866455, -4.392000198364258, -5.038599967956543, -5.038599967956543, -5.038599967956543, -4.663899898529053, -5.038599967956543, -5.038599967956543, -4.002600193023682, -5.038599967956543, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -5.038700103759766, -4.60699987411499, -4.60699987411499, -4.60699987411499, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -5.213200092315674, -3.9602999687194824, -3.9602999687194824, -4.60699987411499, -4.60699987411499, -4.606800079345703, -4.606900215148926, -4.606800079345703, -4.232100009918213, -4.606900215148926, -4.232100009918213, -4.60699987411499, -4.606900215148926, -4.606900215148926, -4.606900215148926, -4.606900215148926, -4.2322998046875, -4.606900215148926, -4.60699987411499, -4.60699987411499, -5.2129998207092285, -5.2129998207092285, -5.213099956512451, -5.213099956512451, -5.213099956512451, -5.213099956512451, -5.213099956512451, -5.213099956512451, -3.5385000705718994, -4.574699878692627, -4.574699878692627, -4.574699878692627, -3.714400053024292, -3.927999973297119, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -5.1809000968933105, -4.574699878692627, -4.57450008392334, -4.5746002197265625, -4.5746002197265625, -3.7142999172210693, -4.199999809265137, -5.180699825286865, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -5.180799961090088, -4.1570000648498535, -4.531700134277344, -4.531700134277344, -4.531700134277344, -4.531700134277344, -4.531700134277344, -3.8847999572753906, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805, -3.4951999187469482, -4.531499862670898, -4.531499862670898, -4.156799793243408, -4.531499862670898, -4.531499862670898, -4.156899929046631, -4.531599998474121, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137700080871582, -5.137800216674805, -5.137800216674805, -5.137800216674805, -5.137800216674805], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.722, 0.6996, 0.6641, 0.6641, 0.6641, 0.6641, 0.6641, 0.6641, 0.6641, 0.6639, 0.6044, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5998, 0.5714, 0.4944, 0.5334, 0.5334, 0.5334, 0.3947, 0.2021, 0.4577, 0.427, 0.2817, 0.3389, 0.325, 0.325, 0.2952, 0.38, 0.2166, 0.3299, 0.2257, -0.3021, -0.2859, 1.2534, 1.1776, 1.1776, 1.1776, 1.1776, 1.1776, 1.1776, 1.1776, 1.1776, 1.1776, 1.1776, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9904, 0.9437, 0.9437, 0.7377, 0.7547, 0.8371, 0.8371, 0.837, 0.6705, 0.6701, 0.6653, -0.0847, 0.5834, 0.5834, 0.5834, 0.168, 0.168, 0.1174, 0.0388, 1.795, 1.795, 1.795, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.5705, 1.3102, 1.3102, 1.4449, 1.4449, 1.3974, 1.3973, 1.2096, 0.6969, 0.9211, 0.2358, 0.7117, 0.5998, 0.5998, 0.579, 0.579, -0.3144, 0.2593, -0.0317, -0.2615, 0.5552, 0.4908, 1.0308, 1.0308, 1.0308, 1.0308, 1.0308, 1.0308, 1.8964, 1.8469, 1.8469, 1.8469, 1.8449, 1.7734, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.6172, 1.491, 1.4428, 1.4428, 1.4427, 0.861, -0.2821, 1.0715, 0.7632, 1.0714, 0.5279, 1.0714, 1.0714, 1.0714, 1.0714, 0.6356, 1.0714, 1.0714, 1.0714, 0.4938, 0.2819, 0.332, 0.107, 1.1329, 1.1329, 2.0237, 1.9178, 1.9178, 1.9178, 1.9178, 1.9178, 1.809, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.6805, 1.372, 1.5044, 1.5043, 0.4185, 0.8271, 0.6683, -0.2391, 0.3947, 0.6306, 1.1262, 1.1262, 1.1262, 0.5435, 1.1262, 1.1262, 1.1262, 0.8822, 0.7715, 0.7715, 0.3751, 1.1884, 1.1884, -0.0603]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 3, 3, 1, 1, 1, 1, 2, 3, 1, 4, 1, 3, 1, 5, 3, 2, 3, 4, 2, 3, 1, 2, 3, 4, 5, 5, 2, 1, 1, 5, 5, 2, 1, 4, 4, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 1, 1, 1, 3, 4, 4, 2, 5, 3, 1, 4, 2, 3, 2, 5, 2, 1, 2, 3, 2, 3, 2, 4, 1, 3, 1, 3, 1, 2, 4, 5, 5, 1, 3, 4, 3, 2, 1, 2, 5, 3, 1, 3, 4, 2, 3, 3, 5, 1, 4, 3, 1, 3, 1, 1, 4, 1, 3, 1, 2, 4, 5, 1, 2, 3, 2, 4, 3, 2, 5, 2, 1, 2, 5, 2, 5, 1, 2, 4, 1, 5, 1, 2, 1, 2, 4, 1, 3, 5, 4, 1, 5, 4, 1, 4, 4, 4, 3, 2, 3, 3, 1, 3, 1, 2, 3, 5, 2, 4, 1, 1, 5, 1, 5, 2, 1, 3, 1, 4, 4, 1, 2, 4, 2, 3, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 1, 3, 4, 4, 2, 4, 1, 2, 1, 4, 5, 1, 2, 3, 3, 2, 5, 3, 3, 1, 3, 2, 4, 1, 2, 3, 1, 1, 4, 5, 4, 1, 3, 5, 1, 5, 1, 3, 1, 4, 3, 2, 5, 1, 2, 4, 2, 3, 1, 2, 2, 5, 2, 1, 2, 5, 1, 3, 1, 2, 3, 5, 1, 2, 3, 1, 5, 1, 5, 5, 1, 1, 2, 5, 5, 3, 4, 1, 2, 3, 4, 5, 2, 4, 4, 4, 1, 2, 1, 3, 2, 5, 5, 4, 4, 1, 4, 1, 2, 4, 1, 4, 2, 3, 2, 2, 5, 1, 2, 3, 4, 4, 2, 1, 4, 1, 5, 5, 1, 3, 1, 3, 1, 5, 1, 4, 1, 2, 1, 1, 2, 1, 2, 4, 1, 2, 4, 5, 2, 4, 1, 2, 2, 2, 1, 3, 2, 2, 4, 3, 1, 5, 3, 1, 2, 4, 5, 1, 2, 1, 4, 1, 1, 1, 2, 1, 3, 4, 5, 3, 2, 1, 2, 3, 5, 2, 4], \"Freq\": [0.5627759069718214, 0.20464578435338962, 0.10232289217669481, 0.10232289217669481, 0.10232289217669481, 0.8986180687802822, 0.6134839587770426, 1.0049463896364472, 1.0049463100208038, 0.7932748849778438, 0.5294666290676602, 0.35297775271177345, 0.17648887635588673, 0.5281414148791779, 0.5281414148791779, 0.5237593520734893, 0.5237593520734893, 0.9137945529066855, 0.9302051845149838, 0.8986180687802822, 0.8064764736200154, 0.4032382368100077, 0.9115928603123882, 0.7557730181745353, 0.8986180798743562, 0.39664515486797897, 0.13221505162265965, 0.13221505162265965, 0.13221505162265965, 0.39664515486797897, 0.6433121924496946, 0.7817654943657455, 0.9137946074687759, 0.5343360352003004, 0.5343360352003004, 0.930205206477513, 1.0189899088654988, 0.4175795255241504, 0.4175795255241504, 0.9115928599290445, 0.9137945620194278, 0.5343367766402323, 0.5343367766402323, 0.9302052037077021, 1.0049463035524346, 0.9302051976663526, 0.9302051760090855, 0.5343413630904336, 0.5343413630904336, 0.930205192777399, 0.9137946411517522, 1.0049463422248293, 0.6978398470888113, 0.2326132823629371, 0.6256430324331362, 0.9115928831458934, 0.7817655666131402, 0.9302051859306938, 0.8986180729152391, 0.5281405803614299, 0.5281405803614299, 0.4322372301853106, 0.4322372301853106, 0.7817655083453421, 0.9302051850237534, 1.0189897344782999, 0.5454872170685138, 0.18182907235617124, 0.18182907235617124, 1.0189898994709197, 0.8986180484739205, 0.7817655877102655, 0.9115929011362102, 0.5237593514689718, 0.5237593514689718, 0.8736777719693308, 0.8986180586271012, 0.4218816228012657, 0.4218816228012657, 0.21094081140063284, 0.9302051792278596, 0.9302051968102044, 0.5935968640752484, 0.39573124271683224, 0.9115928916542435, 0.8986180821275279, 0.7817655857847826, 0.5967339210083802, 0.2983669605041901, 0.2983669605041901, 0.6134839578992979, 0.5237593679470861, 0.5237593679470861, 0.9115928806310598, 0.8064764241524732, 0.4032382120762366, 0.8986180710334539, 0.6433121789594904, 0.5281422980026371, 0.5281422980026371, 0.8986180821275279, 0.5935967987041334, 0.39573119913608895, 0.9137946023662397, 0.5281414202065218, 0.5281414202065218, 0.5237623693077952, 0.5237623693077952, 0.7194074694573018, 0.23980248981910063, 0.23980248981910063, 0.9302051501730023, 0.5454881386923999, 0.1818293795641333, 0.1818293795641333, 0.3044546791372776, 0.6089093582745552, 0.8986180687802822, 0.5686498678266472, 0.5686498678266472, 1.018989814180497, 0.6483008034293569, 0.21610026780978564, 0.21610026780978564, 0.5686498183462902, 0.5686498183462902, 0.5624668123734691, 0.5624668123734691, 0.9115929040428817, 0.5343378561693938, 0.5343378561693938, 0.5245265391728844, 0.5245265391728844, 0.30670045464492107, 0.6134009092898421, 0.30670045464492107, 0.4186473213687093, 0.4186473213687093, 0.4186473213687093, 0.9115928916542435, 0.3021615319769948, 0.6043230639539896, 0.9115928968788831, 0.5281405513321129, 0.5281405513321129, 0.9115929200993593, 0.9115928970766959, 0.8986180840093132, 0.4322372377430113, 0.4322372377430113, 0.8986180586271012, 0.4121337900416439, 0.4121337900416439, 0.3255059990655729, 0.3255059990655729, 0.3255059990655729, 0.3255059990655729, 1.0189898973564933, 0.9115928550858553, 0.9137945632405273, 0.8414862091493871, 0.14024770152489785, 0.42538597377638815, 0.42538597377638815, 1.018989712898823, 0.9137945244475575, 0.8986180484739205, 0.417587529251555, 0.417587529251555, 0.9115929462202044, 0.6040263653782609, 0.20134212179275363, 0.20134212179275363, 0.7817655474382991, 0.8986180811866352, 0.6433122127896248, 0.42537927145869414, 0.42537927145869414, 0.31383393593455555, 0.47075090390183333, 0.07845848398363889, 0.07845848398363889, 0.07845848398363889, 0.36242621329473423, 0.7248524265894685, 0.3415651690895277, 0.3415651690895277, 0.3415651690895277, 0.9115928399995329, 0.5616394638439486, 0.5616394638439486, 0.6718432763152598, 0.22394775877175324, 0.7564371771029865, 0.2521457257009955, 0.2521457257009955, 0.5569327587485006, 0.3712885058323337, 0.18564425291616685, 0.8986180710334539, 0.781765585540775, 0.9302051847184916, 0.6134839652632974, 0.8986180586271012, 0.9137946117237802, 0.8986180462207488, 0.264137072464644, 0.7924112173939322, 0.5119633069538634, 0.2559816534769317, 0.2559816534769317, 0.9137945161364293, 0.4175843024157331, 0.4175843024157331, 0.9302051736443181, 0.9115928701825662, 0.622903864604791, 0.2076346215349303, 0.6433121792050088, 0.5343378510199943, 0.5343378510199943, 0.4121261272181614, 0.4121261272181614, 0.9137945775542639, 0.9115928943649962, 0.8986180830684205, 0.7817655451985219, 0.9302051585859911, 0.9137945889947129, 1.0189897669722716, 0.9115928701798875, 1.0189898076597845, 0.8986180402025249, 0.3624262649814898, 0.7248525299629796, 0.7817655089152079, 0.9833434660236072, 0.7817655530246841, 0.5530907426150239, 0.18436358087167465, 0.18436358087167465, 0.9137945611011598, 0.898618062762058, 0.7094925073535119, 0.08868656341918899, 0.17737312683837797, 0.08868656341918899, 0.5569306470788367, 0.3712870980525578, 0.1856435490262789, 1.0049463768163869, 0.930205152986902, 0.5343359910851521, 0.5343359910851521, 0.9302051563670669, 1.0049462824069055, 0.671843330222181, 0.22394777674072702, 0.9302052121716533, 0.9302052190691511, 0.8986180392616325, 0.9115928649569801, 0.29621197335589344, 0.19747464890392893, 0.09873732445196447, 0.29621197335589344, 0.19747464890392893, 0.4382397500607173, 0.4382397500607173, 0.911592892428478, 0.9115928796637478, 0.9137946555422851, 0.7817654818404509, 0.523760146816493, 0.523760146816493, 0.7817655976346636, 0.9302051799822537, 0.9302051355063126, 0.9115928858530205, 0.9115928432876536, 0.5281405825368793, 0.5281405825368793, 0.5624669821934313, 0.5624669821934313, 0.911592904817116, 0.5281422791031616, 0.5281422791031616, 1.0189898422307737, 0.8986180811866352, 0.781765475545359, 0.7817654779288752, 0.6433121880206427, 0.3052197011731482, 0.6104394023462963, 0.3052197011731482, 0.6256430296952291, 0.911592847544033, 0.7817655947896417, 0.5281414189428073, 0.5281414189428073, 0.9137945875914425, 0.9302051796348753, 0.9302051844132297, 1.0049462609673958, 0.8986180586271012, 0.523762364515923, 0.523762364515923, 0.7496085095513838, 0.3748042547756919, 0.5281471413017601, 0.5281471413017601, 0.29046550128356147, 0.5809310025671229, 0.9137945817095934, 0.6718430746094077, 0.22394769153646923, 0.38805550609366357, 0.38805550609366357, 0.38805550609366357, 0.4894261726055266, 0.3262841150703511, 0.16314205753517555, 0.16314205753517555, 0.5616394715561722, 0.5616394715561722, 0.9148451202597876, 0.1306921600371125, 0.7817654987403285, 0.7817655101954838, 0.9137946662834552, 0.8986180811866352, 0.7817654853975748, 0.23324888698798507, 0.6997466609639552, 0.8986180586271012, 0.7496119738108936, 0.3748059869054468, 0.8986180687802822, 0.59284981361337, 0.296424906806685, 0.296424906806685, 0.9302051795331212, 0.3624243648405373, 0.7248487296810746, 0.9137945496254568, 0.9115928579925264, 0.9137945850388188, 0.9137945602507693, 0.5624673730463023, 0.5624673730463023, 0.5625361485492761, 0.28126807427463807, 0.14063403713731903, 0.14063403713731903, 0.8986180484739205, 0.781765567323741, 0.6603096634749793, 0.1320619326949959, 0.1320619326949959, 0.1320619326949959, 1.018989810501076, 0.6256431116300152], \"Term\": [\"alice\", \"alice\", \"alice\", \"alice\", \"alice\", \"angry\", \"ann\", \"argument\", \"ask\", \"away\", \"began\", \"began\", \"began\", \"begin\", \"begin\", \"better\", \"better\", \"book\", \"bowed\", \"brass\", \"bright\", \"bright\", \"broken\", \"cakes\", \"called\", \"came\", \"came\", \"came\", \"came\", \"came\", \"carrying\", \"case\", \"cat\", \"children\", \"children\", \"clubs\", \"coaxing\", \"come\", \"come\", \"constant\", \"conversations\", \"couldn\", \"couldn\", \"couples\", \"court\", \"courtiers\", \"crimson\", \"crown\", \"crown\", \"cushion\", \"cut\", \"day\", \"dear\", \"dear\", \"deep\", \"deeply\", \"delighted\", \"diamonds\", \"direction\", \"distance\", \"distance\", \"door\", \"door\", \"doubt\", \"doubtful\", \"dream\", \"duchess\", \"duchess\", \"duchess\", \"eat\", \"engraved\", \"enormous\", \"exclamation\", \"executed\", \"executed\", \"executioner\", \"explain\", \"eyes\", \"eyes\", \"eyes\", \"face\", \"faces\", \"fan\", \"fan\", \"fancy\", \"fear\", \"feebly\", \"feet\", \"feet\", \"feet\", \"ferrets\", \"fetch\", \"fetch\", \"finish\", \"frightened\", \"frightened\", \"fur\", \"gardeners\", \"getting\", \"getting\", \"glass\", \"gloves\", \"gloves\", \"going\", \"gone\", \"gone\", \"good\", \"good\", \"got\", \"got\", \"got\", \"grand\", \"great\", \"great\", \"great\", \"gryphon\", \"gryphon\", \"guessed\", \"guests\", \"guests\", \"guinea\", \"hand\", \"hand\", \"hand\", \"hands\", \"hands\", \"hard\", \"hard\", \"hasn\", \"having\", \"having\", \"head\", \"head\", \"hear\", \"hear\", \"hear\", \"heard\", \"heard\", \"heard\", \"heart\", \"hearts\", \"hearts\", \"heavy\", \"help\", \"help\", \"history\", \"hollow\", \"home\", \"house\", \"house\", \"housemaid\", \"hunting\", \"hunting\", \"hurried\", \"hurried\", \"hurried\", \"hurried\", \"idea\", \"interesting\", \"kept\", \"king\", \"king\", \"knave\", \"knave\", \"knee\", \"knew\", \"knocking\", \"know\", \"know\", \"lady\", \"large\", \"large\", \"large\", \"lay\", \"lest\", \"lie\", \"like\", \"like\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lizard\", \"lizard\", \"ll\", \"ll\", \"ll\", \"lonely\", \"long\", \"long\", \"look\", \"look\", \"looked\", \"looked\", \"looked\", \"looking\", \"looking\", \"looking\", \"lost\", \"lovely\", \"manner\", \"mary\", \"meet\", \"mind\", \"mistake\", \"mock\", \"mock\", \"moment\", \"moment\", \"moment\", \"near\", \"nearly\", \"nearly\", \"noticing\", \"occasional\", \"oh\", \"oh\", \"ornamented\", \"ought\", \"ought\", \"pair\", \"pair\", \"party\", \"patiently\", \"paws\", \"pebbles\", \"people\", \"pictures\", \"pigs\", \"pitied\", \"plan\", \"plate\", \"poor\", \"poor\", \"possibly\", \"procession\", \"puppy\", \"queen\", \"queen\", \"queen\", \"question\", \"quick\", \"rabbit\", \"rabbit\", \"rabbit\", \"rabbit\", \"ran\", \"ran\", \"ran\", \"read\", \"recognised\", \"remember\", \"remember\", \"reply\", \"rest\", \"round\", \"round\", \"royal\", \"rule\", \"run\", \"sad\", \"said\", \"said\", \"said\", \"said\", \"said\", \"sat\", \"sat\", \"saw\", \"saying\", \"scroll\", \"second\", \"seen\", \"seen\", \"set\", \"severely\", \"shaped\", \"sigh\", \"sighing\", \"sir\", \"sir\", \"sister\", \"sister\", \"sit\", \"sitting\", \"sitting\", \"size\", \"slowly\", \"small\", \"smaller\", \"soldiers\", \"soon\", \"soon\", \"soon\", \"sorrow\", \"speak\", \"spite\", \"spoke\", \"spoke\", \"started\", \"stood\", \"stopped\", \"stupid\", \"swim\", \"table\", \"table\", \"talking\", \"talking\", \"tell\", \"tell\", \"thing\", \"thing\", \"things\", \"think\", \"think\", \"thinking\", \"thinking\", \"thinking\", \"thought\", \"thought\", \"thought\", \"thought\", \"till\", \"till\", \"time\", \"time\", \"touch\", \"trees\", \"trial\", \"turned\", \"turning\", \"turtle\", \"turtle\", \"upstairs\", \"use\", \"use\", \"vanished\", \"ve\", \"ve\", \"ve\", \"velvet\", \"voice\", \"voice\", \"waistcoat\", \"wants\", \"wasn\", \"watch\", \"way\", \"way\", \"went\", \"went\", \"went\", \"went\", \"whiskers\", \"whistle\", \"white\", \"white\", \"white\", \"white\", \"wood\", \"words\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 4, 3, 5]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el12081404839855222645483598900\", ldavis_el12081404839855222645483598900_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el12081404839855222645483598900\", ldavis_el12081404839855222645483598900_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el12081404839855222645483598900\", ldavis_el12081404839855222645483598900_data);\n            })\n         });\n}\n</script>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Interactive visualization for topic modeling \n",
    "pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)"
   ]
  }
 ]
}
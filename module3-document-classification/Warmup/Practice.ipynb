{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python370jvsc74a57bd0c751e3dba1c0ae6d681c8fd9e4eb9597bc490d2dc541e0c49930039039a54e24",
   "display_name": "Python 3.7.0 64-bit ('U4-S1-NLP': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Objective 01 - extract text features and use them in classification pipelines\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            sentence  label\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wow... Loved this place.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Crust is not good.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Not tasty and the texture was just nasty.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stopped by during the late May bank holiday of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The selection on the menu was great and so wer...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "# import \n",
    "import pandas as pd\n",
    "\n",
    "# read te locally saved file from the link above\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create the function and target varilable\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# train test split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(750,)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "sentences_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<750x2864 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3051 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# import the  tf-idf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# instantiate and fit the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(2,2))\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "# vectorize the training and testing data\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test = vectorizer.transform(sentences_test)\n",
    "\n",
    "# display the properties of the vectorized text\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.588\n"
     ]
    }
   ],
   "source": [
    "# import the classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate and fit a model\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "print('Accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    2.4s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.611"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define the pipeline\n",
    "pipe = Pipeline([('vect', vectorizer), # Vectorizer\n",
    "                 ('clf', classifier)   # classifier\n",
    "                 ])\n",
    "\n",
    "# define the parameter space for the grid search\n",
    "parameters = {'clf__C': [1, 10, 1000000]} # C: regularization strength\n",
    "\n",
    "# implement a grid search with cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(sentences, y);\n",
    "\n",
    "# print out the best score\n",
    "grid_search.best_score_"
   ]
  },
  {
   "source": [
    "## Objective 02 - apply latent semantic indexing (LSA) to a document classification problem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# read teh locally saved file\n",
    "\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()\n",
    "\n",
    "# create the features and target\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# instantiate the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(2,2))\n",
    "\n",
    "# instantiate the classifier (defaults)\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Instantiate the LSA (SVD) algorithm (defaults)\n",
    "svd = TruncatedSVD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  20 | elapsed:    2.4s remaining:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5959999999999999"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# create the pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# LSA Part\n",
    "lsa = Pipeline([('vect', vectorizer),\n",
    "                ('svd', svd)])\n",
    "\n",
    "# combine into one pipeline\n",
    "pipe = Pipeline([('lsa', lsa),\n",
    "                 ('clf', classifier)])\n",
    "\n",
    "# define the parameter space for the grid search\n",
    "parameters = {\n",
    "    'lsa__vect__max_df': (0.9,1.0), # max document frequency\n",
    "    'lsa__svd__n_components': (100,250),\n",
    "}\n",
    "\n",
    "# implement a grid search with cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(sentences, y)\n",
    "\n",
    "# Display the best score from the grid-search\n",
    "grid_search.best_score_"
   ]
  },
  {
   "source": [
    "## Objective 03 - benchmark and compare various vectorization methods in document classification tasks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spaCy and the large pretrained model(includes word embedding)\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            sentence  label\n",
       "0                        Wow... Loved this place.\\t1    NaN\n",
       "1                              Crust is not good.\\t0    NaN\n",
       "2       Not tasty and the texture was just nasty.\\t0    NaN\n",
       "3  Stopped by during the late May bank holiday of...    NaN\n",
       "4  The selection on the menu was great and so wer...    NaN"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wow... Loved this place.\\t1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Crust is not good.\\t0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Not tasty and the texture was just nasty.\\t0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stopped by during the late May bank holiday of...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The selection on the menu was great and so wer...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read the file locally saved\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\\n')\n",
    "df_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy including word embeddings:  0.852\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read in the locally saved file from UCI website\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
    "df_yelp.head()\n",
    "\n",
    "# Create the features and target\n",
    "sentences = df_yelp['sentence']\n",
    "y = df_yelp['label']\n",
    "\n",
    "# Train-test split\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "    sentences, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Function to return the vector for each sentence in a document\n",
    "def get_word_vectors(docs):\n",
    "    return [nlp(doc).vector for doc in docs]\n",
    "\n",
    "# Get the vectors for each sentence (mean of all the word vectors)\n",
    "X_train = get_word_vectors(sentences_train)\n",
    "X_test = get_word_vectors(sentences_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the classifier (defaults)\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "# Print out the accuracy score\n",
    "print(\"Accuracy including word embeddings: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}